{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we open  the file with notebook instead of excel, we will see that numbers are separated by commas\n",
    "# so using np (unlike pd), we can load as a text file so we need to indicate the the comma is the delimiter\n",
    "raw_csv_data = np.loadtxt ('F:\\\\schulich\\\\python\\\\Udemy Data course\\\\required data files\\\\Audiobooks_data.csv', delimiter= ',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1- Extract the raw input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in our data set we do not need the first and the last column.\n",
    "# the first is an arbitary ID for each customer, no useful information. raw_csv_data[:, 0]\n",
    "# the last column is target: raw_csv_data[:, -1]\n",
    "# so we exclude these two columns and we are left with raw input\n",
    "# note that unlike panda, as we do not have the title name of columns, we only have to indicate the index number.\n",
    " \n",
    "unscaled_inputs_all = raw_csv_data[:, 1:-1]\n",
    "\n",
    "targets_all = raw_csv_data[:, -1]\n",
    "\n",
    "targets_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- Balance the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we count all the 1s in our target data set, and take the same number of 0s and remove the rest\n",
    "\n",
    "# Count how many targets are 1 (meaning that the customer did convert)\n",
    "number_of_targets_1 = int(np.sum(targets_all)) # we use int otherwise it gives us a float number\n",
    "\n",
    "# keep as many 0s as 1s\n",
    "zero_targets_counter = 0\n",
    "\n",
    "# we will need a variable that records the indices to be removed. which is empty for now. but we want it to ne a list or tuple\n",
    "indices_to_remove = []\n",
    "\n",
    "# lets itrate over the data set and balance it\n",
    "for i in range(targets_all.shape[0]):  # targets_all.shape[0] = gives the number of all our targets = number of observations\n",
    "    if targets_all[i] == 0:\n",
    "        zero_targets_counter = zero_targets_counter + 1 \n",
    "        if zero_targets_counter > number_of_targets_1: # once we get the same number of 0s, remove the rest\n",
    "            indices_to_remove.append(i)\n",
    "            \n",
    "# as we defined the indices of targets with the value of zero that need to be removed we can remove them\n",
    "unscaled_inputs_equal_priors = np.delete(unscaled_inputs_all, indices_to_remove, axis=0)\n",
    "\n",
    "# similarily we need to apply the same procedure to balance our target \n",
    "targets_equal_priors = np.delete(targets_all, indices_to_remove, axis=0)\n",
    "\n",
    "# now we have a balanced dataset including both balanced input data and target data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3- Standardize the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4474"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the current input is unscaled and we know standarizing will improve our machine learning very well. \n",
    "# That's the only place we use sklearn functionality. We will take advantage of its preprocessing capabilities\n",
    "# It's a simple line of code, which standardizes the inputs, as we explained in one of the lectures.\n",
    "# At the end of the business case, you can try to run the algorithm WITHOUT this line of code. \n",
    "# The result will be interesting.\n",
    "scaled_input = preprocessing.scale(unscaled_inputs_equal_priors)\n",
    "scaled_input.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4- Shuffle the data including inputs and the targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 830, 3337, 1518, ..., 1811, 2758, 2373])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# as it is possible that our original data set is oredered in someway, such as date\n",
    "# since we will batching we need to shuffle the data to have differnt data with no specific thing in each batch\n",
    "\n",
    "# as we want to shuffle our inputs and targets the same way so that each target associated with each input remain correct\n",
    "# so instead of shuffling inputs and targets we define an order from 0 to the number od our scaled obsevations\n",
    "# then we shuffle the indices and then reorder our inputs and targets based on our shuffled indices\n",
    "shuffled_indices = np.arange(scaled_input.shape[0])\n",
    "np.random.shuffle(shuffled_indices)\n",
    "\n",
    "shuffled_inputs = scaled_input[shuffled_indices]\n",
    "shuffled_targets = targets_equal_priors[shuffled_indices]\n",
    "\n",
    "shuffled_indices\n",
    "# note that we can shuffle data at the beginnig of preprocessing or even after balancing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5- Split data into train, validation, and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4474"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets calculate the total number of our remained observations after balancing\n",
    "sample_count = shuffled_inputs.shape[0]\n",
    "\n",
    "# next we must detemine the size of each three datasets\n",
    "# we use 80% , 10%, 10% split\n",
    "train_percent = 0.8\n",
    "validation_percent = 0.1\n",
    "\n",
    "train_sample_count = int(train_percent * sample_count) # make sure you use int\n",
    "validation_sample_count = int(validation_percent * sample_count)\n",
    "test_sample_count = sample_count - (train_sample_count + validation_sample_count)\n",
    "\n",
    "# as we have the size of each one let's extract them\n",
    "train_inputs = shuffled_inputs[:train_sample_count]\n",
    "train_targets = shuffled_targets[:train_sample_count]\n",
    "\n",
    "validation_inputs = shuffled_inputs[train_sample_count:(train_sample_count+validation_sample_count)]\n",
    "validation_targets = shuffled_targets[train_sample_count:(train_sample_count+validation_sample_count)]\n",
    "\n",
    "test_inputs = shuffled_inputs[(train_sample_count+validation_sample_count):]\n",
    "test_targets = shuffled_targets[(train_sample_count+validation_sample_count):]\n",
    "\n",
    "sample_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1773.0 3579 0.4953897736797988\n",
      "228.0 447 0.5100671140939598\n",
      "236.0 448 0.5267857142857143\n"
     ]
    }
   ],
   "source": [
    "# We balanced our dataset to be 50-50 (for targets 0 and 1), but the training, validation, and test were \n",
    "# taken from a shuffled dataset. Check if they are balanced, too. Note that each time you rerun this code, \n",
    "# you will get different values, as each time they are shuffled randomly.\n",
    "# Normally you preprocess ONCE, so you need not rerun this code once it is done.\n",
    "# If you rerun this whole sheet, the npzs will be overwritten with your newly preprocessed data.\n",
    "\n",
    "# let's check the numbers\n",
    "# Print the number of targets that are 1s, the total number of samples, and the proportion for training, validation, and test.\n",
    "print(np.sum(train_targets), train_sample_count, np.sum(train_targets) / train_sample_count)\n",
    "print(np.sum(validation_targets), validation_sample_count, np.sum(validation_targets) / validation_sample_count)\n",
    "print(np.sum(test_targets), test_sample_count, np.sum(test_targets) / test_sample_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6- save the three datasets in *.npz\n",
    "which is compatible with our ML library (tensorflow) that we are going to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the three datasets in *.npz.\n",
    "# try to name them in a very semantic way\n",
    "# In the next lesson, you will see that it is extremely valuable to name them in such a coherent way!\n",
    "np.savez('Audiobooks_data_train', inputs = train_inputs, targets = train_targets)\n",
    "np.savez('Audiobooks_data_validation', inputs = validation_inputs, targets = validation_targets)\n",
    "np.savez('Audiobooks_data_test', inputs = test_inputs, targets = test_targets)\n",
    "# the files have been saved in the same route as this jupyter codes\n",
    "# :C:\\Users\\sasan\\Udemy data science bootcamp 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
